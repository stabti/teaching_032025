{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9C0Ot7jGPWf9S7SHGww1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stabti/teaching_032025/blob/main/1_text_extraction_from_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préambule\n",
        "Ce notebook a pour objectif d'extraire du texte d'un fichier PDF via différents outils et les comparer. On va commencer par installer les packages dont on a besoin et importer les packages Python qui permettront de réaliser les opérations nécéssaires à cela."
      ],
      "metadata": {
        "id": "HqnNuF5Z9etq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVIOHbqO_qp_",
        "outputId": "75677484-db4b-4324-a085-a134ff1232e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFPaThBYMcQe",
        "outputId": "d0de6250-3a1a-40f0-a53a-c6fb03d8688e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.25.2-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling) (4.13.3)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2025.1.31)\n",
            "Collecting docling-core<3.0.0,>=2.19.0 (from docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading docling_core-2.21.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting docling-ibm-models<4.0.0,>=3.4.0 (from docling)\n",
            "  Downloading docling_ibm_models-3.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docling-parse<4.0.0,>=3.3.0 (from docling)\n",
            "  Downloading docling_parse-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting easyocr<2.0,>=1.7 (from docling)\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.11/dist-packages (from docling) (0.28.1)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (5.3.1)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (11.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (2.10.6)\n",
            "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from docling) (2.32.3)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.13.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from docling) (4.67.1)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.12.2)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (4.23.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (6.0.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.9.0)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.19.0->docling) (4.48.3)\n",
            "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (1.26.4)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (4.11.0.86)\n",
            "Requirement already satisfied: safetensors<1,>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from safetensors[torch]<1,>=0.4.3->docling-ibm-models<4.0.0,>=3.4.0->docling) (0.5.3)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (0.20.1+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
            "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (2.0.7)\n",
            "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (24.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.3.0->docling)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4.0.0,>=3.4.0->docling) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (2.18.0)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.21.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0.0,>=2.2.2->docling-ibm-models<4.0.0,>=3.4.0->docling) (3.0.2)\n",
            "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting dill>=0.3.9 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading docling-2.25.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.7/145.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.21.2-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.4.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, filetype, XlsxWriter, rtree, python-dotenv, python-docx, pypdfium2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, dill, python-pptx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, typer, pydantic-settings, nvidia-cusolver-cu12, semchunk, docling-core, docling-parse, easyocr, docling-ibm-models, docling\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed XlsxWriter-3.2.2 dill-0.3.9 docling-2.25.2 docling-core-2.21.2 docling-ibm-models-3.4.1 docling-parse-3.4.0 easyocr-1.7.2 filetype-1.2.0 jsonlines-3.1.0 jsonref-1.1.0 latex2mathml-3.77.0 marko-2.1.2 mpire-2.10.2 multiprocess-0.70.17 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 pydantic-settings-2.8.1 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.1.2 python-dotenv-1.0.1 python-pptx-1.0.2 rtree-1.4.0 semchunk-2.2.2 typer-0.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from google.colab import files\n",
        "from docling.document_converter import DocumentConverter"
      ],
      "metadata": {
        "id": "RSXQYwToHLnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On sélectionne un document PDF stocké en local qu'on souhaiterai charger pour parser le texte qu'il contient (attention à ne pas choisir un document contenant des données sensibles)"
      ],
      "metadata": {
        "id": "0SB0IP2DJNGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# The uploaded file will be stored in a dictionary\n",
        "file_name = list(uploaded.keys())[0]  # Get the name of the uploaded file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "Bliwu94h9ccM",
        "outputId": "174837ec-26f3-4c8f-8e71-61828066354a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2670d8d-ac7a-41fc-80e6-fe47d8af55fe\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2670d8d-ac7a-41fc-80e6-fe47d8af55fe\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CV_LIH.pdf to CV_LIH.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extraction de texte d'un PDF via pymupdf\n",
        "Voici une fonction qui extrait le texte d'un document en PDF natif très rapidement grâce à pymupdf"
      ],
      "metadata": {
        "id": "JNXR7M7IJq6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_text_from_pdf(file_name):\n",
        "  \"\"\"Extracts text from a PDF file using PyMuPDF.\n",
        "\n",
        "  Args:\n",
        "    file_name: The name of the uploaded PDF file.\n",
        "\n",
        "  Returns:\n",
        "    The extracted text from the PDF.\n",
        "  \"\"\"\n",
        "  doc = fitz.open(file_name)  # Open the uploaded file directly\n",
        "  text = \"\"\n",
        "  for page in doc:\n",
        "    text += page.get_text()\n",
        "  return text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lEJvfHFHDnI",
        "outputId": "63d175eb-4f9f-4392-a17e-5845e00822a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+33 (0) 767146924\n",
            "Bordeaux\n",
            "lu.iglesiash@gmail.comLuis IGLESIAS HERNANDEZ\n",
            "Lead ML Engineer & Data Scientist\n",
            "Linked in\n",
            "SKILLS\n",
            "Generative AI\n",
            "Bedrock, Vertex AI, Mistral, OpenAI, LoRa, Constitutional AI, Transformer\n",
            "Data Visualisation\n",
            "Quicksight, PowerBI, Seaborn\n",
            "Coding Languages\n",
            "Python, C, SQL, Terraform, Java, Matlab, Typescript\n",
            "Frameworks\n",
            "Tensorflow, SKLearn, Pytorch, Spark, Pandas, Polars\n",
            "MLOps / Cloud\n",
            "Databricks, CI/CD, AWS, Azure, Docker, Airflow, Infrastructure as Code\n",
            "Databases\n",
            "ORM, PostgreSQL, MySQL, NoSQL, Vector Databases\n",
            "Management\n",
            "Team Management, Project management, Agile\n",
            "Research Highlights\n",
            "2 Patents, 5 Articles, 8 International Conferences, Best Thesis Award,\n",
            "Over 400 Citations\n",
            "Human Languages\n",
            "Spanish (Native) English (C2), French (C2), Italian (B1), Japanese (N4)\n",
            "TECHNICAL EXPERIENCE\n",
            "Machine Learning Engineer / Car Batteries\n",
            "Sept 2024 — Present\n",
            "Automotive Cells Company\n",
            "France, Bordeaux\n",
            "Context\n",
            "As the only Machine Learning Engineer at ACC, I direct the development and deployment of advanced\n",
            "machine learning solutions using Databricks.\n",
            "Responsibilities\n",
            "• Development of CICD for deployment both of backend and frontend.\n",
            "• Model management and deployment for models of different types including computer vision models and\n",
            "LLMs in Databricks.\n",
            "• Infrastructure as code development.\n",
            "• Orchestration workflows for real time and batch predictions\n",
            "• Enforcing of best practices such as unit testing and clean code.\n",
            "• Code architecture design for agile development.\n",
            "• Service monitoring.\n",
            "Lead MLOPS & Data Scientist / Tire Analysis\n",
            "Promoted Feb 2023 — Sept 2024\n",
            "Data Scientist / Tire Analysis\n",
            "Apr 2022 — Feb 2023\n",
            "CaRool - Startup\n",
            "France, Remote\n",
            "Context\n",
            "Lead with Agile (Scrum) methodology, a team of 6 computer vision specialists to develop and deploy\n",
            "machine learning algorithms to production in a multi-cloud environment (AWS and GCP) and assessing the\n",
            "system’s accuracy and reliability. The objective is to measure the tire wear rate from an image and use OCR to\n",
            "read the tire dimensions, brand and other attributes to suggest a tire for replacement.\n",
            "Responsibilities\n",
            "• Preparing the team’s roadmap.\n",
            "• Participation in meetings with core clients\n",
            "• Apply algorithms to asses the wear rate of a tire and extract information to suggest a replacement tire.\n",
            "+33 (0) 767146924\n",
            "Bordeaux\n",
            "lu.iglesiash@gmail.comLuis IGLESIAS HERNANDEZ\n",
            "Lead ML Engineer & Data Scientist\n",
            "Linked in\n",
            "• Architect, develop, deploy and monitor the service built by our team.\n",
            "• Develop and maintain our CI/CD pipeline\n",
            "• Team’s training and coordination\n",
            "Researcher / Signal Processing for Gas Sensors\n",
            "Promoted Oct 2020 — Mar 2022\n",
            "PhD Researcher / Signal Processing for Gas Sensors\n",
            "Sep 2017 — Sep 2020\n",
            "Laboratoire IMS\n",
            "France, Bordeaux\n",
            "Context\n",
            "Signal processing for Smart hydrogen ultrasonic sensor to prevent explosions in nuclear facilities. Designed\n",
            "algorithm that combined signals from multiple sensors to measure hydrogen concentration reliably and\n",
            "robustly. 2 patents, 5 articles in top tier journals, 8 international conferences, Best thesis award\n",
            "Data Scientist / Fall Risk Assessment\n",
            "Returned (CDD) Jan 2017 — Jul 2017\n",
            "Data Scientist Internship / Neuromorphic Computing\n",
            "Apr 2015 — Jun 2015\n",
            "IBM\n",
            "USA, CA - Silicon Valley\n",
            "Context\n",
            "Fall risk assessment after a hip surgery based on motion sensors (accelerometer, gyroscope and magnetic\n",
            "field). Simulation of brain inspired memories to run machine learning algorithms efficiently.\n",
            "Master Thesis / 3D Printing of Animal Cells\n",
            "Mar 2016 — Sep 2016\n",
            "Harvard - MIT\n",
            "USA, MA - Boston\n",
            "Context\n",
            "In charge of designing the electronic circuit and synchronization algorithm of a 3D printer capable of printing\n",
            "organs layer by layer with animal cells.\n",
            "EDUCATION\n",
            "PhD in Electrical Engineering / Signal Processing, Université de Bordeaux\n",
            "Sep 2020\n",
            "Triple Diplôme d’Ingénieur / Master’s in Nanotechnology for Integrated Circuits, EPFL (CH), Grenoble INP -\n",
            "Phelma (FR), Politecnico di Torino (IT)\n",
            "Sep 2016\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lançons la fonction écrite dans la cellule précédente sur le pdf uploadé et analysons le résultat"
      ],
      "metadata": {
        "id": "33PLVwhTKK9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from the PDF\n",
        "extracted_text = extract_text_from_pdf(file_name)\n",
        "\n",
        "# Print the extracted text (or process it further)\n",
        "print(extracted_text)"
      ],
      "metadata": {
        "id": "yUqSMCucKGMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extraction de texte via docling"
      ],
      "metadata": {
        "id": "1cj8vF3MKtiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = DocumentConverter()\n",
        "result = converter.convert(file_name)\n",
        "print(result.document.export_to_markdown())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mx4AtJ-RcUv",
        "outputId": "b6d41139-86b9-4ce2-9455-b4319c575cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n",
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+33 (0) 767146924 Bordeaux\n",
            "\n",
            "## Luis IGLESIAS HERNANDEZ\n",
            "\n",
            "lu.iglesiash@gmail.com\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "Linked in\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "## Lead ML Engineer &amp; Data Scientist\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "## SKILLS\n",
            "\n",
            "Generative AI\n",
            "\n",
            "Bedrock, Vertex AI, Mistral, OpenAI, LoRa, Constitutional AI, Transformer Quicksight, PowerBI, Seaborn\n",
            "\n",
            "Data Visualisation\n",
            "\n",
            "Coding Languages\n",
            "\n",
            "Python, C, SQL, Terraform, Java, Matlab, Typescript\n",
            "\n",
            "Frameworks\n",
            "\n",
            "Tensorflow, SKLearn, Pytorch, Spark, Pandas, Polars Databricks, CI/CD, AWS, Azure, Docker, Airflow, Infrastructure as Code\n",
            "\n",
            "MLOps / Cloud\n",
            "\n",
            "Databases\n",
            "\n",
            "ORM, PostgreSQL, MySQL, NoSQL, Vector Databases\n",
            "\n",
            "Management\n",
            "\n",
            "Team Management, Project management, Agile\n",
            "\n",
            "Research Highlights\n",
            "\n",
            "2 Patents, 5 Articles, 8 International Conferences, Best Thesis Award, Over 400 Citations\n",
            "\n",
            "HumanLanguages\n",
            "\n",
            "Spanish (Native) English (C2), French (C2), Italian (B1), Japanese (N4)\n",
            "\n",
            "## TECHNICAL EXPERIENCE\n",
            "\n",
            "## Machine Learning Engineer / Car Batteries\n",
            "\n",
            "Automotive Cells Company\n",
            "\n",
            "Sept 2024 - Present\n",
            "\n",
            "France, Bordeaux\n",
            "\n",
            "## Context\n",
            "\n",
            "As the only Machine Learning Engineer at ACC, I direct the development and deployment of advanced machine learning solutions using Databricks.\n",
            "\n",
            "## Responsibilities\n",
            "\n",
            "- · Development of CICD for deployment both of backend and frontend.\n",
            "- · Infrastructure as code development.\n",
            "- · Model management and deployment for models of different types including computer vision models and LLMs in Databricks.\n",
            "- · Orchestration workflows for real time and batch predictions\n",
            "- · Code architecture design for agile development.\n",
            "- · Enforcing of best practices such as unit testing and clean code.\n",
            "- · Service monitoring.\n",
            "\n",
            "Lead MLOPS &amp; Data Scientist / Tire Analysis\n",
            "\n",
            "Promoted Feb 2023 - Sept 2024 Apr 2022 - Feb 2023\n",
            "\n",
            "Data Scientist / Tire Analysis\n",
            "\n",
            "CaRool - Startup\n",
            "\n",
            "France, Remote\n",
            "\n",
            "## Context\n",
            "\n",
            "Lead with Agile (Scrum) methodology, a team of 6 computer vision specialists to develop and deploy machine learning algorithms to production in a multi-cloud environment (AWS and GCP) and assessing the system's accuracy and reliability. The objective is to measure the tire wear rate from an image and use OCR to read the tire dimensions, brand and other attributes to suggest a tire for replacement.\n",
            "\n",
            "## Responsibilities\n",
            "\n",
            "- · Preparing the team's roadmap.\n",
            "- · Apply algorithms to asses the wear rate of a tire and extract information to suggest a replacement tire.\n",
            "- · Participation in meetings with core clients\n",
            "\n",
            "+33 (0) 767146924 Bordeaux\n",
            "\n",
            "## Luis IGLESIAS HERNANDEZ\n",
            "\n",
            "lu.iglesiash@gmail.com\n",
            "\n",
            "Lead ML Engineer &amp; Data Scientist\n",
            "\n",
            "- · Architect, develop, deploy and monitor the service built by our team.\n",
            "- · Team's training and coordination\n",
            "\n",
            "Linked in\n",
            "\n",
            "- · Develop and maintain our CI/CD pipeline\n",
            "\n",
            "Researcher / Signal Processing for Gas Sensors\n",
            "\n",
            "Promoted Oct 2020 - Mar 2022\n",
            "\n",
            "PhD Researcher / Signal Processing for Gas Sensors\n",
            "\n",
            "Sep 2017 - Sep 2020\n",
            "\n",
            "Laboratoire IMS\n",
            "\n",
            "France, Bordeaux\n",
            "\n",
            "## Context\n",
            "\n",
            "Signal processing for Smart hydrogen ultrasonic sensor to prevent explosions in nuclear facilities. Designed algorithm that combined signals from multiple sensors to measure hydrogen concentration reliably and robustly. 2 patents, 5 articles in top tier journals, 8 international conferences, Best thesis award\n",
            "\n",
            "Data Scientist / Fall Risk Assessment\n",
            "\n",
            "Returned (CDD) Jan 2017 - Jul 2017\n",
            "\n",
            "Data Scientist Internship / Neuromorphic Computing\n",
            "\n",
            "Apr 2015 - Jun 2015\n",
            "\n",
            "IBM\n",
            "\n",
            "USA, CA - Silicon Valley\n",
            "\n",
            "Context\n",
            "\n",
            "Fall risk assessment after a hip surgery based on motion sensors (accelerometer, gyroscope and magnetic field). Simulation of brain inspired memories to run machine learning algorithms efficiently.\n",
            "\n",
            "Master Thesis / 3D Printing of Animal Cells\n",
            "\n",
            "Harvard - MIT\n",
            "\n",
            "Mar 2016 - Sep 2016\n",
            "\n",
            "USA, MA - Boston\n",
            "\n",
            "## Context\n",
            "\n",
            "In charge of designing the electronic circuit and synchronization algorithm of a 3D printer capable of printing organs layer by layer with animal cells.\n",
            "\n",
            "## EDUCATION\n",
            "\n",
            "PhD in Electrical Engineering / Signal Processing , Université de Bordeaux Sep 2020 Triple Diplôme d'Ingénieur / Master's in Nanotechnology for Integrated Circuits , EPFL (CH), Grenoble INP -\n",
            "\n",
            "Phelma (FR), Politecnico di Torino (IT)\n",
            "\n",
            "Sep 2016\n"
          ]
        }
      ]
    }
  ]
}