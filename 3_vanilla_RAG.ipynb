{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d780e97124c426882fb5d4699251be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b2cb11b5aad4a9a9144b30801c99b96",
              "IPY_MODEL_6be1e8febfb9412eaaa3af143ab1b11c",
              "IPY_MODEL_9fc9cabd7bf5435b9e36ce7a66125c93"
            ],
            "layout": "IPY_MODEL_54ef3aec5fa743729df6329bf3dfd97a"
          }
        },
        "0b2cb11b5aad4a9a9144b30801c99b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c939c37aa77458cbe6534edb4699b26",
            "placeholder": "​",
            "style": "IPY_MODEL_dd2e44970cd34f4c8eff2dd268f41408",
            "value": "Downloading shards: 100%"
          }
        },
        "6be1e8febfb9412eaaa3af143ab1b11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1789d6db2ad9484ca17857965e6c14a0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1edc3320986a4be78af459212b10d9d2",
            "value": 2
          }
        },
        "9fc9cabd7bf5435b9e36ce7a66125c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13e4c37ca2b4fd08bb078f8535ad35b",
            "placeholder": "​",
            "style": "IPY_MODEL_f5c24df6b4834a22a26333ca442c1a34",
            "value": " 2/2 [00:55&lt;00:00, 23.54s/it]"
          }
        },
        "54ef3aec5fa743729df6329bf3dfd97a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c939c37aa77458cbe6534edb4699b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2e44970cd34f4c8eff2dd268f41408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1789d6db2ad9484ca17857965e6c14a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edc3320986a4be78af459212b10d9d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a13e4c37ca2b4fd08bb078f8535ad35b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c24df6b4834a22a26333ca442c1a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4cd30df9e64bcc947c551f42f53fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09e8bc60c857475e954b37da1c0d1bbe",
              "IPY_MODEL_44a75fd5906e4f34b50fddf2073fc265",
              "IPY_MODEL_3f45ace492414591a887314b210ffadb"
            ],
            "layout": "IPY_MODEL_d27b92a1e28d436081439ae9e1b0a4bd"
          }
        },
        "09e8bc60c857475e954b37da1c0d1bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce720963400e4715ac96b6b115d538f7",
            "placeholder": "​",
            "style": "IPY_MODEL_2bdc702496e944cc859cc3f77af5fb88",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "44a75fd5906e4f34b50fddf2073fc265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0104fa9a56284766bb5434fa22c4c43a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a4511e2b96f40cd9afe1a9d44c68492",
            "value": 2
          }
        },
        "3f45ace492414591a887314b210ffadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ac7385835b44f8a01ae7a8c76dc6ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a4c0a35a42ed4c04bfaa48bacca1f405",
            "value": " 2/2 [00:27&lt;00:00, 11.58s/it]"
          }
        },
        "d27b92a1e28d436081439ae9e1b0a4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce720963400e4715ac96b6b115d538f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bdc702496e944cc859cc3f77af5fb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0104fa9a56284766bb5434fa22c4c43a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4511e2b96f40cd9afe1a9d44c68492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85ac7385835b44f8a01ae7a8c76dc6ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c0a35a42ed4c04bfaa48bacca1f405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stabti/teaching_032025/blob/main/3_vanilla_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environnement d'exécution\n",
        "Attention, il faut choisir un environnement T4 (cliquer sur la petite flèche à droite de RAM et Disque en haut à droite)."
      ],
      "metadata": {
        "id": "-iEI9jYn1S-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation des packages\n",
        "Ça peut prendre 2 bonnes minutes, c'est normal."
      ],
      "metadata": {
        "id": "cNnYyh13e7ZB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJBUXOVVT33q",
        "outputId": "fb29afee-3198-43eb-e619-47bda14cc222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docling\n",
            "  Downloading docling-2.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting chonkie\n",
            "  Downloading chonkie-0.5.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling) (4.13.3)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2025.1.31)\n",
            "Collecting docling-core<3.0.0,>=2.19.0 (from docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading docling_core-2.22.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting docling-ibm-models<4.0.0,>=3.4.0 (from docling)\n",
            "  Downloading docling_ibm_models-3.4.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting docling-parse<4.0.0,>=3.3.0 (from docling)\n",
            "  Downloading docling_parse-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting easyocr<2.0,>=1.7 (from docling)\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.11/dist-packages (from docling) (0.28.1)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (5.3.1)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.1.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from docling) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (11.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from docling) (2.10.6)\n",
            "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from docling) (2.32.3)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from docling) (1.14.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from docling) (4.67.1)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tokenizers>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from chonkie) (0.21.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (4.12.2)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (4.23.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.9.0)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<4.0.0,>=3.4.0->docling)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (4.11.0.86)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling) (0.20.1+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (0.25.2)\n",
            "Collecting python-bidi (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling) (2.0.7)\n",
            "Collecting pyclipper (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr<2.0,>=1.7->docling)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1,>=0.23->docling) (2024.10.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.3.0->docling)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.3.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<4.0.0,>=3.4.0->docling) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.19.0->docling-core[chunking]<3.0.0,>=2.19.0->docling) (0.23.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.4->docling) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (2.18.0)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.13.0,>=0.12.5->docling) (0.1.2)\n",
            "Collecting multiprocess>=0.70.15 (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting dill>=0.3.9 (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.19.0->docling)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading docling-2.26.0-py3-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.5/146.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chonkie-0.5.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.22.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.5/100.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.4.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading marko-2.1.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, filetype, XlsxWriter, rtree, python-dotenv, python-docx, pypdfium2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, mpire, marko, latex2mathml, jsonref, jsonlines, faiss-cpu, dill, tiktoken, python-pptx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, typer, pydantic-settings, nvidia-cusolver-cu12, semchunk, docling-core, chonkie, docling-parse, easyocr, docling-ibm-models, docling\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed XlsxWriter-3.2.2 chonkie-0.5.1 dill-0.3.9 docling-2.26.0 docling-core-2.22.0 docling-ibm-models-3.4.1 docling-parse-3.4.0 easyocr-1.7.2 faiss-cpu-1.10.0 filetype-1.2.0 jsonlines-3.1.0 jsonref-1.1.0 latex2mathml-3.77.0 marko-2.1.2 mpire-2.10.2 multiprocess-0.70.17 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyclipper-1.3.0.post6 pydantic-settings-2.8.1 pypdfium2-4.30.1 python-bidi-0.6.6 python-docx-1.1.2 python-dotenv-1.0.1 python-pptx-1.0.2 rtree-1.4.0 semchunk-2.2.2 tiktoken-0.9.0 typer-0.12.5\n"
          ]
        }
      ],
      "source": [
        "!pip install docling chonkie tiktoken sentence-transformers faiss-cpu transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import des packages"
      ],
      "metadata": {
        "id": "EhLwKLK3e0I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docling.document_converter import DocumentConverter # For text extraction\n",
        "from chonkie import TokenChunker # For chunking\n",
        "from sentence_transformers import SentenceTransformer  # For embeddings\n",
        "import faiss  # For vector database\n",
        "import numpy as np  # For numerical operations\n",
        "from google.colab import files # to load a pdf file to test the system\n",
        "\n",
        "# For answer generation with a small language model\n",
        "from transformers import pipeline  # For generating answers with a language model\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "eCqO9j13UdKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement du PDF\n",
        "Un bouton pour sélectionner le fichier à importer apparaît. Il suffit de faire la sélection."
      ],
      "metadata": {
        "id": "cDChhyQz7d8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "# The uploaded file will be stored in a dictionary\n",
        "file_name = list(uploaded.keys())[0]  # Get the name of the uploaded file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ndK7cCFsWzrE",
        "outputId": "af6e3638-d4a0-4399-d7f5-d2c89ca19d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef438528-6bdb-4c78-942d-b993fe6dab26\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef438528-6bdb-4c78-942d-b993fe6dab26\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving evaluation_de_eia_ue1_livet.pdf to evaluation_de_eia_ue1_livet.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction du texte du PDF"
      ],
      "metadata": {
        "id": "KP9yHIDC767L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(file_name,output_file=\"output.md\"):\n",
        "  converter = DocumentConverter()\n",
        "  result = converter.convert(file_name)\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    text=result.document.export_to_markdown()\n",
        "    f.write(text)\n",
        "    return text\n",
        "\n",
        "  print(f\"Markdown content saved to: {output_file}\")"
      ],
      "metadata": {
        "id": "DLmGJTdLX1wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'extraction (réalisée dans la cellule ci-dessous) prend un peu de temps (1mn environ) c'est normal (un modèle d'OCR est chargé pour analyser le document et cela est chronophage). Une fois terminé, on peut regarder le resultat en ouvrant le fichier markdown dans les documents (panneau de gauche)."
      ],
      "metadata": {
        "id": "nLzNPQqA8Goy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Extraction de text avec docling,\n",
        "text = extract_text_from_pdf(\"evaluation_de_eia_ue1_livet.pdf\",\"output.md\")"
      ],
      "metadata": {
        "id": "AmdMoFZFac9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbeb9b78-bdf6-45bd-81b3-6f73a740a053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking\n",
        "On découpe le texte extrait en petits morceaux (chunks) grâce au package Chonkie. Plusieurs techniques de chunking existent et il est bon d'en comparer plusieurs dans les cas d'usages en entreprise. Ici nous utilisons une méthode très simple qui découpe des chunks de taille chunk_size et avec un intersection de chunk_overlap entre chaque chunk."
      ],
      "metadata": {
        "id": "tgwGfxIT8TXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text):\n",
        "  # Initialize the chunker\n",
        "  chunker = TokenChunker(chunk_size=90, chunk_overlap=30) # defaults to using GPT2 tokenizer\n",
        "  # Chunk the text\n",
        "  chunks = chunker(text)\n",
        "  return chunks\n"
      ],
      "metadata": {
        "id": "zUUi1RIAiFEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### tester le chunking\n",
        "chunks= process_text(text)\n",
        "# observer les chunks obtenus\n",
        "for chunk in chunks:\n",
        "    print(f\"Chunk: {chunk.text}\")\n",
        "    print(f\"Tokens: {chunk.token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9QMzSKriigG",
        "outputId": "9abef6e3-0cee-47e8-a5d1-25df8e0001f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: ## Evaluation DE EIA - UE1\n",
            "\n",
            "## Andréas Livet\n",
            "\n",
            "## Introduction\n",
            "\n",
            "Le projet d'IA que je vais décrire est un projet existant sur lequel je travaille en tant que prestataire au sein du ministère de la transition écologique. J'ai choisi de traiter ce projet, car je ne travaille\n",
            "Tokens: 90\n",
            "Chunk:  ministère de la transition écologique. J'ai choisi de traiter ce projet, car je ne travaille pas spécifiquement sur le modèle d'IA et j'avais envie d'en savoir d'avantage sur cette partie. De plus, je pense qu'il illustre parfaitement tous les défis mentionn\n",
            "Tokens: 90\n",
            "Chunk: antage sur cette partie. De plus, je pense qu'il illustre parfaitement tous les défis mentionnés dans les différentes étapes de mise en place d'un projet IA.\n",
            "\n",
            "## Description du produit et de la problématique\n",
            "\n",
            "Le logiciel que nous développons s'appelle pour l'instant « \n",
            "Tokens: 90\n",
            "Chunk:  et de la problématique\n",
            "\n",
            "Le logiciel que nous développons s'appelle pour l'instant « Liriae viewer » et s'intègre dans un projet d'expérimentation de l'IA plus important appelé SofIA. C'est un projet qui vise à fournir à l'ensemble des agents ministériels\n",
            "Tokens: 90\n",
            "Chunk:  appelé SofIA. C'est un projet qui vise à fournir à l'ensemble des agents ministériels un chat bot alimenté par toute la base documentaire présente dans les différents services du ministère, de l'ADEME et du CEREMA.\n",
            "\n",
            "Il peut s'agir de texte de loi, de directives,\n",
            "Tokens: 90\n",
            "Chunk:  l'ADEME et du CEREMA.\n",
            "\n",
            "Il peut s'agir de texte de loi, de directives, de procédures et le fait de pouvoir les interroger dans un langage naturel et obtenir une réponse synthétique et pertinente permettrait aux agents de mieux appréhender certains aspects légaux de\n",
            "Tokens: 90\n",
            "Chunk:  réponse synthétique et pertinente permettrait aux agents de mieux appréhender certains aspects légaux de leur travail et d'être plus efficace dans la rédaction de certains documents.\n",
            "\n",
            "Liriae Viewer vise à fournir un chat bot individualisé sur des collections de documents qui ne font pas partis de la base documentaire SofIA\n",
            "Tokens: 90\n",
            "Chunk: er vise à fournir un chat bot individualisé sur des collections de documents qui ne font pas partis de la base documentaire SofIA et de permettre aussi d'annoter ces documents.\n",
            "\n",
            "Liriae Viewer leur permettrait de téléverser des dossiers d'impact environnementale qui peuvent faire plusieurs centaines de pages,\n",
            "Tokens: 90\n",
            "Chunk: éléverser des dossiers d'impact environnementale qui peuvent faire plusieurs centaines de pages, de pouvoir les annoter via un système de commentaire et de surlignage et de les questionner via un chatbot alimenté dynamiquement avec les dossiers en question. Un peu à l'instar de ce que fait\n",
            "Tokens: 90\n",
            "Chunk:  chatbot alimenté dynamiquement avec les dossiers en question. Un peu à l'instar de ce que fait Notebook LM.\n",
            "\n",
            "Aujourd'hui, les agents environnementaux croulent sous le travail, le traitement des dossiers prend du temps et les retards s'accumulent.\n",
            "\n",
            "A terme, n\n",
            "Tokens: 90\n",
            "Chunk: , le traitement des dossiers prend du temps et les retards s'accumulent.\n",
            "\n",
            "A terme, nous envisageons d'intégrer des fonctionnalités de partage des collections, d'aide à la rédaction   des   avis   et   d'intégrer   la   base   document\n",
            "Tokens: 90\n",
            "Chunk:  rédaction   des   avis   et   d'intégrer   la   base   documentaire   SofIA   pour   notamment   intégrer   les composantes légales dans l'étude des dossiers.\n",
            "\n",
            "Le but est de permettre aux agents environnementaux de rédiger des\n",
            "Tokens: 90\n",
            "Chunk:  l'étude des dossiers.\n",
            "\n",
            "Le but est de permettre aux agents environnementaux de rédiger des avis plus précis et de gagner du temps dans l'analyse des dossiers.\n",
            "\n",
            "Le modèle d'IA est composé d'un LLM non entraîné spécifiquement pour la tâche (\n",
            "Tokens: 90\n",
            "Chunk: le d'IA est composé d'un LLM non entraîné spécifiquement pour la tâche (Mistral 7B) et d'une   infrastructure   maison   pour   rajouter   des   fonctionnalités   de   RAG   (Retrival   Augmented Generation) au chat bot.\n",
            "\n",
            "\n",
            "Tokens: 90\n",
            "Chunk: onctionnalités   de   RAG   (Retrival   Augmented Generation) au chat bot.\n",
            "\n",
            "Certaines   (SofIA)   sont   récupéré   en   amont   et   traitées   à   l'avance,   d'autres   sont   récupérés\n",
            "Tokens: 90\n",
            "Chunk:   traitées   à   l'avance,   d'autres   sont   récupérés dynamiquement et traiter à la demande.\n",
            "\n",
            "Les données nécessaires sont principalement des fichiers pdf agrémentés de méta données (voir plus loin).\n",
            "\n",
            "L'outil s'utilise\n",
            "Tokens: 90\n",
            "Chunk:  pdf agrémentés de méta données (voir plus loin).\n",
            "\n",
            "L'outil s'utilise comme un chatbot classique, il prend en entrée une question de l'utilisateur et sort une réponse au format texte agrémenté d'une liste de source qui correspond aux extraits en lien avec la problématique. Com\n",
            "Tokens: 90\n",
            "Chunk:  texte agrémenté d'une liste de source qui correspond aux extraits en lien avec la problématique. Comme avec Notebook LM, ces liens sont cliquables, on peut soit juste voir l'extrait, soit aller à la page du document pour l'annoter si besoin. L'historique des conversation est sauvegard\n",
            "Tokens: 90\n",
            "Chunk: , soit aller à la page du document pour l'annoter si besoin. L'historique des conversation est sauvegardé à l'instar de ce qui se fait dans lechat ou chatGPT.\n",
            "\n",
            "## Etapes de mise en place du projet\n",
            "\n",
            "Bien que j'ai rejoins le projet dans sa phase de réalisation et d'exp\n",
            "Tokens: 90\n",
            "Chunk:  en place du projet\n",
            "\n",
            "Bien que j'ai rejoins le projet dans sa phase de réalisation et d'expérimentation, je vais tâcher de décrire les différentes phases de conception tel que je les ai comprises et telles qu'elles m'ont été rapporté par Bruno Lenzi, chef de projet data et IA au se\n",
            "Tokens: 90\n",
            "Chunk:  comprises et telles qu'elles m'ont été rapporté par Bruno Lenzi, chef de projet data et IA au sein du ministère. Je tiens à le remercier pour le temps d'échange que nous avons eu à ce sujet.\n",
            "\n",
            "## Phase de cadrage\n",
            "\n",
            "Le projet d'inscrit dans un processus d'expé\n",
            "Tokens: 90\n",
            "Chunk:  à ce sujet.\n",
            "\n",
            "## Phase de cadrage\n",
            "\n",
            "Le projet d'inscrit dans un processus d'expérimentation sur l'IA au sein du ministère et plus globalement au sein de l'appareil d'état.\n",
            "\n",
            "Une équipe est en place (SofIA) et plusieurs prestataires l'épaulent\n",
            "Tokens: 90\n",
            "Chunk: at.\n",
            "\n",
            "Une équipe est en place (SofIA) et plusieurs prestataires l'épaulent ponctuellement, l'infrastructure cloud est gérer par les équipes IT de l'ADEME qui peuvent être en support des équipes sur des questions de déploiement.\n",
            "\n",
            "Le projet est  donc  pr\n",
            "Tokens: 90\n",
            "Chunk: vent être en support des équipes sur des questions de déploiement.\n",
            "\n",
            "Le projet est  donc  prévu   pour   évoluer   en   fonction   des   besoins   qui   émergeront   des   premiers prototypes.\n",
            "\n",
            "Les besoins en terme humains et techniques ont pu �\n",
            "Tokens: 90\n",
            "Chunk:   émergeront   des   premiers prototypes.\n",
            "\n",
            "Les besoins en terme humains et techniques ont pu être cadrés et défini, pour les données cela a été plus problématique.\n",
            "\n",
            "## La collecte des données\n",
            "\n",
            "Le projet SofIA cherchant à centraliser les données de plusieurs\n",
            "Tokens: 90\n",
            "Chunk: ## La collecte des données\n",
            "\n",
            "Le projet SofIA cherchant à centraliser les données de plusieurs services, il a fallu dialoguer avec les experts de la base documentaire (sorte de « data owner ») de chaque service.\n",
            "\n",
            "Il n'y a pas de gouvernance centralisée sur les données et chacun à sa\n",
            "Tokens: 90\n",
            "Chunk: que service.\n",
            "\n",
            "Il n'y a pas de gouvernance centralisée sur les données et chacun à sa manière de les stocker. Aussi il n'y a pas de point d'accès dynamique aux données. L'équipe SofIA reçoit, un fichier tabulaire contenant les métadonné\n",
            "Tokens: 90\n",
            "Chunk: es. L'équipe SofIA reçoit, un fichier tabulaire contenant les métadonnées des fichiers ainsi qu'un lien pour les télécharger. L'équipe doit donc périodiquement récupérer la liste des fichiers de chaque service et les ajouter manuellement à\n",
            "Tokens: 90\n",
            "Chunk:  périodiquement récupérer la liste des fichiers de chaque service et les ajouter manuellement à sa base.\n",
            "\n",
            "## Exploration des données\n",
            "\n",
            "Dans l'ensemble, les métadonnées étaient cohérentes avec les fichiers, en revanche, il a fallu demander aux services d'uniformiser\n",
            "Tokens: 90\n",
            "Chunk: taient cohérentes avec les fichiers, en revanche, il a fallu demander aux services d'uniformiser les noms de champs et les formats (format de date par exemple).\n",
            "\n",
            "- « On passe des millions d'heures à travers le monde pour parser un format de fichier propriétaire (pdf) pour reconstituer, approximative\n",
            "Tokens: 90\n",
            "Chunk: heures à travers le monde pour parser un format de fichier propriétaire (pdf) pour reconstituer, approximativement, des données qu'on a dans le document initial ! », Bruno Lenzi\n",
            "\n",
            "Une fois les données récupérées avec leurs bonnes métadonnées, il fallait les pré\n",
            "Tokens: 90\n",
            "Chunk:  les données récupérées avec leurs bonnes métadonnées, il fallait les préparer pour la vectorisation. Ce travail n'est pas évident car un document au format PDF ne contient pas de données sémantiques indiquant la structure du document (titre, paragraphe, colonnes, tableau\n",
            "Tokens: 90\n",
            "Chunk: nées sémantiques indiquant la structure du document (titre, paragraphe, colonnes, tableau etc.) il faut dont « déduire » toutes ces infos et le choix a été fait de faire de la reconnaissance de caractères (OCR) sur toutes les pages (mêmes celles au format text\n",
            "Tokens: 90\n",
            "Chunk: ait de faire de la reconnaissance de caractères (OCR) sur toutes les pages (mêmes celles au format textuel) et de reconstruire la suite de texte à l'aide d'outil sophistiqués (notamment un petit modèle de langage).\n",
            "\n",
            "Il n'y a pas eu de train/split sur les données v\n",
            "Tokens: 90\n",
            "Chunk:  petit modèle de langage).\n",
            "\n",
            "Il n'y a pas eu de train/split sur les données vu qu'il n'y a pas eu d'entraînement sur le modèle de langage.\n",
            "\n",
            "## Labellisation des données\n",
            "\n",
            "Les   fichiers   comportaient   déjà   des\n",
            "Tokens: 90\n",
            "Chunk:  Labellisation des données\n",
            "\n",
            "Les   fichiers   comportaient   déjà   des   métadonnées   et   il   n'a   pas   été   nécessaire   d'en   rajouter manuellement.\n",
            "\n",
            "## Modélisation\n",
            "\n",
            "Comme expl\n",
            "Tokens: 90\n",
            "Chunk:  nécessaire   d'en   rajouter manuellement.\n",
            "\n",
            "## Modélisation\n",
            "\n",
            "Comme expliqué plus haut, le modèle d'IA utilisé est un LLM Mistral 7B auquel s'ajoute une base de données vectorielle (qdrant) contenant les extraits des documents ingérés qui sont ensu\n",
            "Tokens: 90\n",
            "Chunk:  une base de données vectorielle (qdrant) contenant les extraits des documents ingérés qui sont ensuite interrogés pour extraire les éléments correspondants à la question de l'utilisateur. L'outil utilisé pour piloter ces divers éléments est le framework python « langchain ».\n",
            "\n",
            "Un prompt système spéc\n",
            "Tokens: 90\n",
            "Chunk:  piloter ces divers éléments est le framework python « langchain ».\n",
            "\n",
            "Un prompt système spécifique est donnée au LLM pour lui éviter de répondre a des questions ne relevant pas du travail des agents environnementaux.\n",
            "\n",
            "## Déploiement, Tests\n",
            "\n",
            "La phase de déploiement a ét\n",
            "Tokens: 90\n",
            "Chunk:  des agents environnementaux.\n",
            "\n",
            "## Déploiement, Tests\n",
            "\n",
            "La phase de déploiement a été assez difficile, car toute l'infrastructure est en « infra as code » sur un cluster kubernetes supervisé par Argo. Le souci c'est que l'équipe SofIA était dépendante de\n",
            "Tokens: 90\n",
            "Chunk: netes supervisé par Argo. Le souci c'est que l'équipe SofIA était dépendante de l'infra mise en place par l'IT de l'ADEME et qu'il a fallu beaucoup de temps et d'énergie pour mettre au point l'ensemble des procédures permettant le déploiement autom\n",
            "Tokens: 90\n",
            "Chunk: ps et d'énergie pour mettre au point l'ensemble des procédures permettant le déploiement automatisé (Dockerfile, fichier gitlab   CI,   configurations   spécifiques   etc.).   Clairement,   il   fallait   des   compétences   poussées  \n",
            "Tokens: 90\n",
            "Chunk:    Clairement,   il   fallait   des   compétences   poussées   en devops dont nous manquions dans l'équipe.\n",
            "\n",
            "Lors de nos tests, il y a eu beaucoup de problèmes avec le traitement sur GPU, notamment certains gros fichiers (env 750 pages\n",
            "Tokens: 90\n",
            "Chunk:  beaucoup de problèmes avec le traitement sur GPU, notamment certains gros fichiers (env 750 pages) font exploser la mémoire des serveurs et arrête le processus.\n",
            "\n",
            "De plus,  la   vectorisation   des   fichiers   se   faisant   sur   des   GPU   loués\n",
            "Tokens: 90\n",
            "Chunk: isation   des   fichiers   se   faisant   sur   des   GPU   loués   sur   des   plages   horaires spécifiques, il a fallu gérer le redémarrage des tâches qui avaient mal commencées ou qui étaient en erreur.\n",
            "\n",
            "C\n",
            "Tokens: 90\n",
            "Chunk: marrage des tâches qui avaient mal commencées ou qui étaient en erreur.\n",
            "\n",
            "Ces problématiques techniques, principalement liées à l'infrastructure ont eu un effet assez négatif sur la vision que les utilisateurs béta testeurs avaient sur le projet.\n",
            "\n",
            "Il y a tout de\n",
            "Tokens: 90\n",
            "Chunk:  sur la vision que les utilisateurs béta testeurs avaient sur le projet.\n",
            "\n",
            "Il y a tout de même eu plusieurs retours encourageants et certains utilisateurs pensent intégrer l'outil dans leur travail.\n",
            "\n",
            "Comme dit plus haut, ce sont des personnes déjà surchargées de\n",
            "Tokens: 90\n",
            "Chunk:  leur travail.\n",
            "\n",
            "Comme dit plus haut, ce sont des personnes déjà surchargées de travail qui ne veulent pas passer trop de temps à tester des outils qui ne fonctionne pas bien.\n",
            "\n",
            "## Maintenance, réentrainement\n",
            "\n",
            "Le projet va évoluer en fonction des retours utilisateurs dans une\n",
            "Tokens: 90\n",
            "Chunk: \n",
            "## Maintenance, réentrainement\n",
            "\n",
            "Le projet va évoluer en fonction des retours utilisateurs dans une démarche « agile » où nous cherchons à proposer les fonctionnalités les plus importantes en priorité.\n",
            "\n",
            "Nous espérons aussi que les documents d'impact environnementale puissent\n",
            "Tokens: 90\n",
            "Chunk:  importantes en priorité.\n",
            "\n",
            "Nous espérons aussi que les documents d'impact environnementale puissent aussi être récupérés de manière automatique afin de permettre, comme pour les documents SofIA un traitement en amont, mais cela implique la mise en place d'une procédure de dépô\n",
            "Tokens: 90\n",
            "Chunk: IA un traitement en amont, mais cela implique la mise en place d'une procédure de dépôt de dossier via un site internet qui est toujours au stade expérimental.\n",
            "\n",
            "Au niveau du modèle IA, nous envisageons d'utiliser des modèles de langage spécialisés pour les tâches de\n",
            "Tokens: 90\n",
            "Chunk: le IA, nous envisageons d'utiliser des modèles de langage spécialisés pour les tâches de RAG et plus petits que l'actuel et de revoir la phase de traitement des fichiers pour les rendre plus fiable et moins gourmande (peut-être en ne prenant pas tout le fichier d'un coup).\n",
            "Tokens: 90\n",
            "Chunk: iable et moins gourmande (peut-être en ne prenant pas tout le fichier d'un coup). Idéalement, il faudrait pouvoir « allumer » les GPU que quand il y en a besoin au lieu de les louer sur des plages horaires ce qui coûte cher.\n",
            "\n",
            "## Conformité RGPD et AI Act\n",
            "Tokens: 90\n",
            "Chunk:  au lieu de les louer sur des plages horaires ce qui coûte cher.\n",
            "\n",
            "## Conformité RGPD et AI Act\n",
            "\n",
            "L'application utilise le service gouvernementale d'authentification centralisé « Pro Connect » et ne stocke que l'email, le nom et le prénom de la personne qui est de plus fonctionnaire d'état.\n",
            "Tokens: 90\n",
            "Chunk: ke que l'email, le nom et le prénom de la personne qui est de plus fonctionnaire d'état. Ces informations sont donc déjà connues de la plateforme et ne constituent pas des informations personnelles sensibles.\n",
            "\n",
            "Il y a pour l'instant un mécanisme d'effacement de l'historique des conversations, m\n",
            "Tokens: 90\n",
            "Chunk: \n",
            "\n",
            "Il y a pour l'instant un mécanisme d'effacement de l'historique des conversations, mais pas encore d'effacement de compte. Cela reste possible via une API Rest.\n",
            "\n",
            "Concernant l'AI Act, notre projet se définirai plutôt comme un système d'IA vu qu'\n",
            "Tokens: 90\n",
            "Chunk: AI Act, notre projet se définirai plutôt comme un système d'IA vu qu'il combine plusieurs composants logiciels autour de plusieurs modèles de langage et qu'il est conçu pour une tâche spécifique qui n'est pas lié à des activités considérées à risque\n",
            "Tokens: 90\n",
            "Chunk:  pour une tâche spécifique qui n'est pas lié à des activités considérées à risque par l'IA act (police, contrôle des frontières, scoring social etc.).\n",
            "\n",
            "Un autre risque éventuel serait que le système de RAG ne renvoi pas toujours les données les plus pertinentes par\n",
            "Tokens: 90\n",
            "Chunk: uel serait que le système de RAG ne renvoi pas toujours les données les plus pertinentes par rapport à la question posée, dans ce cas un agent pourrait manquer des informations essentielles à l'analyse de ses dossiers et s'il se base uniquement sur le chatbot pour l'analyse du dossier, être\n",
            "Tokens: 90\n",
            "Chunk:  de ses dossiers et s'il se base uniquement sur le chatbot pour l'analyse du dossier, être biaisé dans la rédaction de son avis.\n",
            "\n",
            "Je dirai que le système se situe dans la catégorie des risques limités (Limited Risks), car utilisant un LLM il n'est pas exempt d\n",
            "Tokens: 90\n",
            "Chunk: ans la catégorie des risques limités (Limited Risks), car utilisant un LLM il n'est pas exempt d'erreurs factuelles, même si le mécanisme de RAG réduit fortement cette possibilité.\n",
            "Tokens: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyBCQwb5UWs7",
        "outputId": "05c97c87-9181-463d-ab14-0fa08cc19c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chunks: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval system\n",
        "On utilise un modèle d'embeddings issu de la librairie transformers de huggingface pour transformer chaque chunk en un vecteur numérique. On va indexer et stocker ces vecteurs grâce à la librairie FAISS développée par Meta qui facilitera également la recherche de vecteurs similaires dans la suite."
      ],
      "metadata": {
        "id": "Vp5PByXC86Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create retrieval system (FAISS index) with embeddings\n",
        "def create_retriever(chunks):\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    embeddings = model.encode(chunks, convert_to_tensor=True)\n",
        "    embeddings_np = embeddings.cpu().numpy()\n",
        "\n",
        "    dimension = embeddings_np.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings_np)\n",
        "    return index, model"
      ],
      "metadata": {
        "id": "a4R_PhNbjW9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create retrieval system\n",
        "index, model = create_retriever(chunks)"
      ],
      "metadata": {
        "id": "aP8ptKA9mb5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identification des chunks les plus pertinents par rapport à la query"
      ],
      "metadata": {
        "id": "UXNGrrlR-A40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant chunks based on query\n",
        "def retrieve_info(index, model, query):\n",
        "    # transformer la query en vecteur d'embedding\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
        "    # identifier les k chunks les plus pertinents pour cette query\n",
        "    k = 3  # Number of nearest neighbors to retrieve\n",
        "    scores, indices = index.search(query_embedding.reshape(1, -1), k)\n",
        "    return scores, indices\n",
        "\n",
        "# Example usage\n",
        "# query = \"votre demande en français ici\"  # Replace with your French query\n",
        "# scores, indices = retrieve_info(index, model, query)"
      ],
      "metadata": {
        "id": "88DfzdaIlW16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query: Question à poser au système\n",
        "query= \"Pour quelle raison on s'intéresse à l'utilisation de l'intelligence artificielle dans ce projet ?\""
      ],
      "metadata": {
        "id": "kIHOStVcmxzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant chunks\n",
        "scores, indices = retrieve_info(index, model, query)"
      ],
      "metadata": {
        "id": "fx0oAFMFnMnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regrouper les chunks les plus pertinents an un seul text qui constituera\n",
        "# le context du prompt du modèle de language qui va générer une réponse à la question\n",
        "retrieved_text = \" \".join([chunks[idx].text for idx in indices[0]])\n",
        "#answer = generate_answer(retrieved_text, query)\n",
        "#print(\"\\nAnswer:\", answer)"
      ],
      "metadata": {
        "id": "OIGeTawfnbMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBbt48J0ApSK",
        "outputId": "e4971ac2-70f4-40cd-d706-c371e42dc107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nées sémantiques indiquant la structure du document (titre, paragraphe, colonnes, tableau etc.) il faut dont « déduire » toutes ces infos et le choix a été fait de faire de la reconnaissance de caractères (OCR) sur toutes les pages (mêmes celles au format text netes supervisé par Argo. Le souci c'est que l'équipe SofIA était dépendante de l'infra mise en place par l'IT de l'ADEME et qu'il a fallu beaucoup de temps et d'énergie pour mettre au point l'ensemble des procédures permettant le déploiement autom éléverser des dossiers d'impact environnementale qui peuvent faire plusieurs centaines de pages, de pouvoir les annoter via un système de commentaire et de surlignage et de les questionner via un chatbot alimenté dynamiquement avec les dossiers en question. Un peu à l'instar de ce que fait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Génération d'une réponse à la question (query)\n",
        "Grâce à un small language model, en utilisant les chunks les plus pertinents par rapport à la question (contexte) et en formulant un prompt qui concatène ce contexte et la question"
      ],
      "metadata": {
        "id": "CZd2JTGEAghG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement du modèle de language, ça peut prendre jusqu'à 1 minute environ\n",
        "\n",
        "model_name = \"ibm-granite/granite-3.1-2b-instruct\"\n",
        "# Chargement du tokenizer pour traiter le texte du prompt\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "language_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"balanced\",  # Using balanced CPU mapping.\n",
        "    torch_dtype=torch.float16  # Use float16 if supported.\n",
        ")\n",
        "# mettre le modèle en mode \"inférence\"\n",
        "language_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532,
          "referenced_widgets": [
            "9d780e97124c426882fb5d4699251be2",
            "0b2cb11b5aad4a9a9144b30801c99b96",
            "6be1e8febfb9412eaaa3af143ab1b11c",
            "9fc9cabd7bf5435b9e36ce7a66125c93",
            "54ef3aec5fa743729df6329bf3dfd97a",
            "0c939c37aa77458cbe6534edb4699b26",
            "dd2e44970cd34f4c8eff2dd268f41408",
            "1789d6db2ad9484ca17857965e6c14a0",
            "1edc3320986a4be78af459212b10d9d2",
            "a13e4c37ca2b4fd08bb078f8535ad35b",
            "f5c24df6b4834a22a26333ca442c1a34",
            "dc4cd30df9e64bcc947c551f42f53fc9",
            "09e8bc60c857475e954b37da1c0d1bbe",
            "44a75fd5906e4f34b50fddf2073fc265",
            "3f45ace492414591a887314b210ffadb",
            "d27b92a1e28d436081439ae9e1b0a4bd",
            "ce720963400e4715ac96b6b115d538f7",
            "2bdc702496e944cc859cc3f77af5fb88",
            "0104fa9a56284766bb5434fa22c4c43a",
            "2a4511e2b96f40cd9afe1a9d44c68492",
            "85ac7385835b44f8a01ae7a8c76dc6ce",
            "a4c0a35a42ed4c04bfaa48bacca1f405"
          ]
        },
        "id": "Dr8QvI5ts7U9",
        "outputId": "3f874046-c2f3-4d57-cd8c-e14aa73fb4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d780e97124c426882fb5d4699251be2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc4cd30df9e64bcc947c551f42f53fc9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraniteForCausalLM(\n",
              "  (model): GraniteModel(\n",
              "    (embed_tokens): Embedding(49155, 2048, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-39): 40 x GraniteDecoderLayer(\n",
              "        (self_attn): GraniteAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): GraniteMLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): GraniteRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): GraniteRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): GraniteRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): GraniteRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=49155, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#device = \"auto\"\n",
        "#chat = [\n",
        "#    { \"role\": \"user\", \"content\":retrieved_text+\"\\n\"+query},\n",
        "#]\n",
        "\n",
        "chat = [\n",
        "        { \"role\": \"user\", \"content\": f\"Based on the following information, answer the question: \\n\\n{retrieved_text}\\n\\n{query}\" },\n",
        "    ]\n",
        "chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "# tokenize the text\n",
        "input_tokens = tokenizer(chat, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "# generate output tokens\n",
        "output = language_model.generate(**input_tokens,max_new_tokens=512)\n",
        "# decode output tokens into text\n",
        "output = tokenizer.batch_decode(output)\n",
        "# print output\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDCQgcxVyW8x",
        "outputId": "3ac9559b-a27f-4172-f038-c20b9abbd41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\\nToday's Date: March 13, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>Based on the following information, answer the question: \\n\\nnées sémantiques indiquant la structure du document (titre, paragraphe, colonnes, tableau etc.) il faut dont «\\xa0déduire\\xa0» toutes ces infos et le choix a été fait de faire de la reconnaissance de caractères (OCR) sur toutes les pages (mêmes celles au format text netes supervisé par Argo. Le souci c'est que l'équipe SofIA était dépendante de l'infra mise en place par l'IT de l'ADEME et qu'il a fallu beaucoup de temps et d'énergie pour mettre au point l'ensemble des procédures permettant le déploiement autom éléverser des dossiers d'impact environnementale qui peuvent faire plusieurs centaines de pages, de pouvoir les annoter via un système de commentaire et de surlignage et de les questionner via un chatbot alimenté dynamiquement avec les dossiers en question. Un peu à l'instar de ce que fait\\n\\nPour quelle raison on s'intéresse à l'utilisation de l'intelligence artificielle dans ce projet?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>L'intérêt pour l'utilisation de l'intelligence artificielle (IA) dans ce projet réside dans plusieurs aspects clés :\\n\\n1. Automatisation : L'IA permet d'automatiser le processus de reconnaissance de caractères (OCR) sur toutes les pages, y compris celles au format texte net supervisé par Argo. Cela évite le besoin manuel de transcription et réduit les erreurs potentielles.\\n\\n2. Efficacité : En annonçant et en surlignant les documents d'impact environnemental, l'IA permet une analyse plus rapide et plus efficace des dossiers, qui peuvent contenir plusieurs centaines de pages.\\n\\n3. Interactivité : L'IA permet de créer un système de commentaire et de surlignage, ainsi qu'un chatbot alimenté dynamiquement avec les dossiers en question. Cela facilite l'interaction avec les documents et permet une meilleure compréhension et extraction des informations.\\n\\n4. Scalabilité : L'IA peut gérer des volumes importants de documents, ce qui est essentiel pour les dossiers d'impact environnemental qui peuvent faire plusieurs centaines de pages.\\n\\n5. Réduction des coûts et des délais : En automatisant les tâches et en améliorant l'efficacité, l'IA peut contribuer à réduire les coûts et les délais associés à l'analyse et à l'annotation des dossiers d'impact environnemental.\\n\\nEn résumé, l'utilisation de l'IA dans ce projet vise à améliorer l'automatisation, l'efficacité, l'interactivité, la scalabilité et à réduire les coûts et les délais associés à l'analyse et à l'annotation des dossiers d'impact environnemental.<|end_of_text|>\"]\n"
          ]
        }
      ]
    }
  ]
}